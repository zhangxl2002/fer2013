{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "acc143c3",
      "metadata": {
        "id": "acc143c3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision.transforms as transforms\n",
        "import datetime\n",
        "from matplotlib.animation import FuncAnimation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aq4l8oZgsm7",
        "outputId": "5479c4d5-b7c7-4018-a0c3-4cd6b1ebca8b"
      },
      "id": "3aq4l8oZgsm7",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VPLFYssblq25"
      },
      "id": "VPLFYssblq25",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d63de53e",
      "metadata": {
        "id": "d63de53e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "55d61bd9-ff51-473e-fd84-a53d57ed3f85"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-51d5d2cc80ef>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# 使用示例\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdata_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/icml_face_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m# train_features, train_labels, public_test_features, public_test_labels, private_test_features, private_test_labels = csv_to_tensor(data_file_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-51d5d2cc80ef>\u001b[0m in \u001b[0;36mcsv_to_tensor\u001b[0;34m(csvfile_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcsv_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# 读取数据文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 提取像素列并转换为适当的格式\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "def csv_to_tensor(csvfile_path):\n",
        "    # 读取数据文件\n",
        "    data_df = pd.read_csv(csvfile_path)\n",
        "\n",
        "    # 提取像素列并转换为适当的格式\n",
        "    data_df['pixels'] = data_df['pixels'].apply(lambda x: np.array([int(pixel) for pixel in x.split()]).reshape(1, 48, 48))\n",
        "\n",
        "    # 划分数据集为训练集和测试集\n",
        "    train_df = data_df[data_df['Usage'] == 'Training']\n",
        "    test_df = data_df[data_df['Usage'] != 'Training']\n",
        "\n",
        "    train_features = [torch.tensor(feature, dtype=torch.float32) for feature in train_df['pixels'].values]\n",
        "    train_features = torch.stack(train_features)/255\n",
        "    train_labels = torch.tensor(train_df['emotion'].values, dtype=torch.int64)\n",
        "    test_features = [torch.tensor(feature, dtype=torch.float32) for feature in test_df['pixels'].values]\n",
        "    test_features = torch.stack(test_features)/255\n",
        "    test_labels = torch.tensor(test_df['emotion'].values, dtype=torch.int64)\n",
        "\n",
        "    # public_test_df = data_df[data_df['Usage'] == 'PublicTest']\n",
        "    # public_test_features = [torch.tensor(feature, dtype=torch.float32) for feature in public_test_df['pixels'].values]\n",
        "    # public_test_features = torch.stack(public_test_features)/255\n",
        "    # public_test_labels = torch.tensor(public_test_df['emotion'].values, dtype=torch.int64)\n",
        "\n",
        "    # private_test_df = data_df[data_df['Usage'] == 'PrivateTest']\n",
        "    # private_test_features = [torch.tensor(feature, dtype=torch.float32) for feature in private_test_df['pixels'].values]\n",
        "    # private_test_features = torch.stack(private_test_features)/255\n",
        "    # private_test_labels = torch.tensor(private_test_df['emotion'].values, dtype=torch.int64)\n",
        "\n",
        "    return train_features, train_labels, test_features, test_labels\n",
        "    # return train_features, train_labels, public_test_features, public_test_labels, private_test_features, private_test_labels\n",
        "\n",
        "# 使用示例\n",
        "data_file_path = '/content/drive/MyDrive/Colab Notebooks/icml_face_data.csv'\n",
        "train_features, train_labels, test_features, test_labels = csv_to_tensor(data_file_path)\n",
        "# train_features, train_labels, public_test_features, public_test_labels, private_test_features, private_test_labels = csv_to_tensor(data_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0da1fa21",
      "metadata": {
        "id": "0da1fa21"
      },
      "outputs": [],
      "source": [
        "def add_random_noise(image, mean=0, std=0.1):\n",
        "    noise = torch.randn_like(image) * std + mean\n",
        "    noisy_image = image + noise\n",
        "    noisy_image = torch.clamp(noisy_image, 0, 1)\n",
        "    return noisy_image\n",
        "def enlarge_trainset(train_features, train_labels, transform):\n",
        "    augmented_train_features = torch.stack([transform(train_features[i]) for i in range(train_features.size(0))])\n",
        "    # return (\n",
        "    #     torch.cat((train_features, augmented_train_features), dim = 0),\n",
        "    #     torch.cat((train_labels, train_labels), dim = 0),\n",
        "    # )\n",
        "    return (\n",
        "        augmented_train_features,\n",
        "        train_labels\n",
        "    )\n",
        "transform = transforms.Compose([lambda x: add_random_noise(x),\n",
        "                                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                            ])\n",
        "\n",
        "# transform = transforms.Compose([transforms.RandomErasing(0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
        "#                                 transforms.RandomHorizontalFlip(p=0.5),\n",
        "#                             ])\n",
        "# train_features, train_labels = enlarge_trainset(train_features, train_labels, transform)\n",
        "# print(train_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b10a3563",
      "metadata": {
        "id": "b10a3563",
        "outputId": "9bc6615c-f299-4216-ebff-9befbd9ae596",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "5680d77f",
      "metadata": {
        "id": "5680d77f"
      },
      "outputs": [],
      "source": [
        "# a=16\n",
        "# b=32\n",
        "a=16\n",
        "b=32\n",
        "c=120\n",
        "d=64\n",
        "# 定义LeNet模型\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, a, kernel_size=5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(a, b, kernel_size=5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(b * 9 * 9, c)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.fc2 = nn.Linear(c, d)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(d, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, b * 9 * 9)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu4(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 输入通道数1，输出通道数64，卷积核大小3x3\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 128, kernel_size=3, padding=1),  # 输入通道数64，输出通道数128，卷积核大小3x3\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128 * 12 * 12, 256),  # 全连接层\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256, 7)  # 输出层，7个表情类别\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ox8mHdHR3smf"
      },
      "id": "ox8mHdHR3smf",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化空的训练和测试准确率列表以及epochs列表\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "epochs = []"
      ],
      "metadata": {
        "id": "zpMebGB5mrHg"
      },
      "id": "zpMebGB5mrHg",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "BE2bf23t54mc"
      },
      "id": "BE2bf23t54mc",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "20b1fbfe",
      "metadata": {
        "id": "20b1fbfe",
        "outputId": "1d1be79e-5fd5-4fa0-f71f-98535d72abc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Accuracy on testset: 24.71%\n",
            "Accuracy on trainset: 25.13%\n",
            "Epoch [1/200], Loss: 1.6715\n",
            "Accuracy on testset: 33.27%\n",
            "Accuracy on trainset: 33.02%\n",
            "Epoch [2/200], Loss: 1.6188\n",
            "Accuracy on testset: 38.66%\n",
            "Accuracy on trainset: 37.91%\n",
            "Epoch [3/200], Loss: 1.4341\n",
            "Accuracy on testset: 41.11%\n",
            "Accuracy on trainset: 40.54%\n",
            "Epoch [4/200], Loss: 1.3891\n",
            "Accuracy on testset: 41.93%\n",
            "Accuracy on trainset: 41.63%\n",
            "Epoch [5/200], Loss: 1.5433\n",
            "Accuracy on testset: 44.47%\n",
            "Accuracy on trainset: 44.39%\n",
            "Epoch [6/200], Loss: 1.5641\n",
            "Accuracy on testset: 44.80%\n",
            "Accuracy on trainset: 45.10%\n",
            "Epoch [7/200], Loss: 1.3449\n",
            "Accuracy on testset: 47.07%\n",
            "Accuracy on trainset: 47.90%\n",
            "Epoch [8/200], Loss: 1.4326\n",
            "Accuracy on testset: 47.41%\n",
            "Accuracy on trainset: 48.84%\n",
            "Epoch [9/200], Loss: 1.2600\n",
            "Accuracy on testset: 47.59%\n",
            "Accuracy on trainset: 49.43%\n",
            "Epoch [10/200], Loss: 1.3670\n",
            "Accuracy on testset: 49.14%\n",
            "Accuracy on trainset: 51.09%\n",
            "Epoch [11/200], Loss: 1.4055\n",
            "Accuracy on testset: 48.59%\n",
            "Accuracy on trainset: 50.74%\n",
            "Epoch [12/200], Loss: 1.3980\n",
            "Accuracy on testset: 50.10%\n",
            "Accuracy on trainset: 52.92%\n",
            "Epoch [13/200], Loss: 1.2521\n",
            "Accuracy on testset: 49.99%\n",
            "Accuracy on trainset: 53.25%\n",
            "Epoch [14/200], Loss: 1.4488\n",
            "Accuracy on testset: 50.95%\n",
            "Accuracy on trainset: 54.56%\n",
            "Epoch [15/200], Loss: 1.2029\n",
            "Accuracy on testset: 51.43%\n",
            "Accuracy on trainset: 55.73%\n",
            "Epoch [16/200], Loss: 1.2276\n",
            "Accuracy on testset: 51.48%\n",
            "Accuracy on trainset: 56.23%\n",
            "Epoch [17/200], Loss: 1.5351\n",
            "Accuracy on testset: 52.30%\n",
            "Accuracy on trainset: 57.60%\n",
            "Epoch [18/200], Loss: 1.3630\n",
            "Accuracy on testset: 51.67%\n",
            "Accuracy on trainset: 57.65%\n",
            "Epoch [19/200], Loss: 1.3032\n",
            "Accuracy on testset: 52.51%\n",
            "Accuracy on trainset: 58.69%\n",
            "Epoch [20/200], Loss: 0.9779\n",
            "Accuracy on testset: 52.05%\n",
            "Accuracy on trainset: 57.93%\n",
            "Epoch [21/200], Loss: 1.1140\n",
            "Accuracy on testset: 52.49%\n",
            "Accuracy on trainset: 59.01%\n",
            "Epoch [22/200], Loss: 1.2261\n",
            "Accuracy on testset: 53.05%\n",
            "Accuracy on trainset: 60.54%\n",
            "Epoch [23/200], Loss: 1.1313\n",
            "Accuracy on testset: 51.91%\n",
            "Accuracy on trainset: 59.07%\n",
            "Epoch [24/200], Loss: 1.1184\n",
            "Accuracy on testset: 53.48%\n",
            "Accuracy on trainset: 61.37%\n",
            "Epoch [25/200], Loss: 1.3210\n",
            "Accuracy on testset: 53.48%\n",
            "Accuracy on trainset: 62.55%\n",
            "Epoch [26/200], Loss: 1.3493\n",
            "Accuracy on testset: 53.72%\n",
            "Accuracy on trainset: 62.38%\n",
            "Epoch [27/200], Loss: 1.1648\n",
            "Accuracy on testset: 53.91%\n",
            "Accuracy on trainset: 63.96%\n",
            "Epoch [28/200], Loss: 1.3250\n",
            "Accuracy on testset: 54.25%\n",
            "Accuracy on trainset: 64.10%\n",
            "Epoch [29/200], Loss: 1.3794\n",
            "Accuracy on testset: 53.97%\n",
            "Accuracy on trainset: 62.84%\n",
            "Epoch [30/200], Loss: 1.1361\n",
            "Accuracy on testset: 53.43%\n",
            "Accuracy on trainset: 63.41%\n",
            "Epoch [31/200], Loss: 1.1341\n",
            "Accuracy on testset: 53.80%\n",
            "Accuracy on trainset: 63.41%\n",
            "Epoch [32/200], Loss: 1.1585\n",
            "Accuracy on testset: 54.17%\n",
            "Accuracy on trainset: 65.21%\n",
            "Epoch [33/200], Loss: 1.0067\n",
            "Accuracy on testset: 54.28%\n",
            "Accuracy on trainset: 65.97%\n",
            "Epoch [34/200], Loss: 1.1203\n",
            "Accuracy on testset: 54.40%\n",
            "Accuracy on trainset: 66.58%\n",
            "Epoch [35/200], Loss: 0.8532\n",
            "Accuracy on testset: 54.61%\n",
            "Accuracy on trainset: 67.59%\n",
            "Epoch [36/200], Loss: 1.0003\n",
            "Accuracy on testset: 55.17%\n",
            "Accuracy on trainset: 68.06%\n",
            "Epoch [37/200], Loss: 1.4298\n",
            "Accuracy on testset: 55.07%\n",
            "Accuracy on trainset: 67.86%\n",
            "Epoch [38/200], Loss: 1.2885\n",
            "Accuracy on testset: 54.61%\n",
            "Accuracy on trainset: 67.14%\n",
            "Epoch [39/200], Loss: 1.0945\n",
            "Accuracy on testset: 55.04%\n",
            "Accuracy on trainset: 68.42%\n",
            "Epoch [40/200], Loss: 1.1521\n",
            "Accuracy on testset: 55.28%\n",
            "Accuracy on trainset: 68.47%\n",
            "Epoch [41/200], Loss: 1.1515\n",
            "Accuracy on testset: 55.06%\n",
            "Accuracy on trainset: 69.92%\n",
            "Epoch [42/200], Loss: 1.0939\n",
            "Accuracy on testset: 55.45%\n",
            "Accuracy on trainset: 69.19%\n",
            "Epoch [43/200], Loss: 1.1146\n",
            "Accuracy on testset: 55.71%\n",
            "Accuracy on trainset: 70.39%\n",
            "Epoch [44/200], Loss: 1.3524\n",
            "Accuracy on testset: 55.85%\n",
            "Accuracy on trainset: 70.63%\n",
            "Epoch [45/200], Loss: 1.0595\n",
            "Accuracy on testset: 55.45%\n",
            "Accuracy on trainset: 70.87%\n",
            "Epoch [46/200], Loss: 0.8802\n",
            "Accuracy on testset: 54.97%\n",
            "Accuracy on trainset: 71.77%\n",
            "Epoch [47/200], Loss: 1.0868\n",
            "Accuracy on testset: 55.52%\n",
            "Accuracy on trainset: 71.65%\n",
            "Epoch [48/200], Loss: 0.9418\n",
            "Accuracy on testset: 55.36%\n",
            "Accuracy on trainset: 71.86%\n",
            "Epoch [49/200], Loss: 1.1959\n",
            "Accuracy on testset: 56.23%\n",
            "Accuracy on trainset: 72.06%\n",
            "Epoch [50/200], Loss: 1.0660\n",
            "Accuracy on testset: 55.25%\n",
            "Accuracy on trainset: 71.73%\n",
            "Epoch [51/200], Loss: 0.9678\n",
            "Accuracy on testset: 55.52%\n",
            "Accuracy on trainset: 72.70%\n",
            "Epoch [52/200], Loss: 1.0192\n",
            "Accuracy on testset: 55.78%\n",
            "Accuracy on trainset: 73.52%\n",
            "Epoch [53/200], Loss: 1.1434\n",
            "Accuracy on testset: 55.66%\n",
            "Accuracy on trainset: 73.34%\n",
            "Epoch [54/200], Loss: 1.2691\n",
            "Accuracy on testset: 56.00%\n",
            "Accuracy on trainset: 73.44%\n",
            "Epoch [55/200], Loss: 0.8221\n",
            "Accuracy on testset: 55.63%\n",
            "Accuracy on trainset: 73.94%\n",
            "Epoch [56/200], Loss: 0.8809\n",
            "Accuracy on testset: 56.20%\n",
            "Accuracy on trainset: 74.25%\n",
            "Epoch [57/200], Loss: 0.9864\n",
            "Accuracy on testset: 55.74%\n",
            "Accuracy on trainset: 74.35%\n",
            "Epoch [58/200], Loss: 0.9565\n",
            "Accuracy on testset: 56.20%\n",
            "Accuracy on trainset: 74.20%\n",
            "Epoch [59/200], Loss: 1.4033\n",
            "Accuracy on testset: 55.63%\n",
            "Accuracy on trainset: 74.68%\n",
            "Epoch [60/200], Loss: 1.2017\n",
            "Accuracy on testset: 55.43%\n",
            "Accuracy on trainset: 74.57%\n",
            "Epoch [61/200], Loss: 1.2694\n",
            "Accuracy on testset: 55.45%\n",
            "Accuracy on trainset: 74.79%\n",
            "Epoch [62/200], Loss: 0.9885\n",
            "Accuracy on testset: 55.70%\n",
            "Accuracy on trainset: 75.43%\n",
            "Epoch [63/200], Loss: 0.9699\n",
            "Accuracy on testset: 56.03%\n",
            "Accuracy on trainset: 76.05%\n",
            "Epoch [64/200], Loss: 0.9017\n",
            "Accuracy on testset: 55.61%\n",
            "Accuracy on trainset: 74.45%\n",
            "Epoch [65/200], Loss: 0.7486\n",
            "Accuracy on testset: 56.38%\n",
            "Accuracy on trainset: 75.45%\n",
            "Epoch [66/200], Loss: 0.9693\n",
            "Accuracy on testset: 55.34%\n",
            "Accuracy on trainset: 76.17%\n",
            "Epoch [67/200], Loss: 1.5104\n",
            "Accuracy on testset: 56.63%\n",
            "Accuracy on trainset: 76.69%\n",
            "Epoch [68/200], Loss: 0.8819\n",
            "Accuracy on testset: 56.02%\n",
            "Accuracy on trainset: 76.86%\n",
            "Epoch [69/200], Loss: 1.0744\n",
            "Accuracy on testset: 56.81%\n",
            "Accuracy on trainset: 76.83%\n",
            "Epoch [70/200], Loss: 1.0650\n",
            "Accuracy on testset: 55.73%\n",
            "Accuracy on trainset: 74.30%\n",
            "Epoch [71/200], Loss: 0.8905\n",
            "Accuracy on testset: 56.13%\n",
            "Accuracy on trainset: 77.34%\n",
            "Epoch [72/200], Loss: 0.8997\n",
            "Accuracy on testset: 56.53%\n",
            "Accuracy on trainset: 77.88%\n",
            "Epoch [73/200], Loss: 1.1376\n",
            "Accuracy on testset: 56.45%\n",
            "Accuracy on trainset: 76.73%\n",
            "Epoch [74/200], Loss: 0.9495\n",
            "Accuracy on testset: 56.13%\n",
            "Accuracy on trainset: 77.65%\n",
            "Epoch [75/200], Loss: 1.2386\n",
            "Accuracy on testset: 56.21%\n",
            "Accuracy on trainset: 77.64%\n",
            "Epoch [76/200], Loss: 1.2394\n",
            "Accuracy on testset: 56.32%\n",
            "Accuracy on trainset: 77.02%\n",
            "Epoch [77/200], Loss: 0.8084\n",
            "Accuracy on testset: 56.09%\n",
            "Accuracy on trainset: 77.84%\n",
            "Epoch [78/200], Loss: 1.1386\n",
            "Accuracy on testset: 55.67%\n",
            "Accuracy on trainset: 78.67%\n",
            "Epoch [79/200], Loss: 0.9657\n",
            "Accuracy on testset: 56.27%\n",
            "Accuracy on trainset: 78.02%\n",
            "Epoch [80/200], Loss: 0.6814\n",
            "Accuracy on testset: 55.82%\n",
            "Accuracy on trainset: 77.78%\n",
            "Epoch [81/200], Loss: 0.9030\n",
            "Accuracy on testset: 56.32%\n",
            "Accuracy on trainset: 78.60%\n",
            "Epoch [82/200], Loss: 1.1570\n",
            "Accuracy on testset: 56.60%\n",
            "Accuracy on trainset: 79.92%\n",
            "Epoch [83/200], Loss: 1.0755\n",
            "Accuracy on testset: 56.95%\n",
            "Accuracy on trainset: 78.92%\n",
            "Epoch [84/200], Loss: 0.7997\n",
            "Accuracy on testset: 56.24%\n",
            "Accuracy on trainset: 79.73%\n",
            "Epoch [85/200], Loss: 0.8197\n",
            "Accuracy on testset: 56.65%\n",
            "Accuracy on trainset: 79.78%\n",
            "Epoch [86/200], Loss: 0.8295\n",
            "Accuracy on testset: 56.21%\n",
            "Accuracy on trainset: 78.85%\n",
            "Epoch [87/200], Loss: 0.9244\n",
            "Accuracy on testset: 56.20%\n",
            "Accuracy on trainset: 80.25%\n",
            "Epoch [88/200], Loss: 0.7865\n",
            "Accuracy on testset: 56.12%\n",
            "Accuracy on trainset: 79.44%\n",
            "Epoch [89/200], Loss: 1.0190\n",
            "Accuracy on testset: 56.12%\n",
            "Accuracy on trainset: 80.29%\n",
            "Epoch [90/200], Loss: 1.0792\n",
            "Accuracy on testset: 56.28%\n",
            "Accuracy on trainset: 79.51%\n",
            "Epoch [91/200], Loss: 1.0443\n",
            "Accuracy on testset: 56.20%\n",
            "Accuracy on trainset: 80.76%\n",
            "Epoch [92/200], Loss: 1.0435\n",
            "Accuracy on testset: 57.22%\n",
            "Accuracy on trainset: 81.11%\n",
            "Epoch [93/200], Loss: 1.0080\n",
            "Accuracy on testset: 56.92%\n",
            "Accuracy on trainset: 80.63%\n",
            "Epoch [94/200], Loss: 0.6977\n",
            "Accuracy on testset: 56.63%\n",
            "Accuracy on trainset: 81.19%\n",
            "Epoch [95/200], Loss: 0.9752\n",
            "Accuracy on testset: 56.26%\n",
            "Accuracy on trainset: 80.84%\n",
            "Epoch [96/200], Loss: 0.8660\n",
            "Accuracy on testset: 56.67%\n",
            "Accuracy on trainset: 79.94%\n",
            "Epoch [97/200], Loss: 1.1416\n",
            "Accuracy on testset: 56.97%\n",
            "Accuracy on trainset: 81.41%\n",
            "Epoch [98/200], Loss: 0.9247\n",
            "Accuracy on testset: 56.80%\n",
            "Accuracy on trainset: 81.71%\n",
            "Epoch [99/200], Loss: 0.8368\n",
            "Accuracy on testset: 56.24%\n",
            "Accuracy on trainset: 82.16%\n",
            "Epoch [100/200], Loss: 1.0698\n",
            "Accuracy on testset: 56.85%\n",
            "Accuracy on trainset: 80.88%\n",
            "Epoch [101/200], Loss: 0.9615\n",
            "Accuracy on testset: 57.02%\n",
            "Accuracy on trainset: 81.33%\n",
            "Epoch [102/200], Loss: 0.5937\n",
            "Accuracy on testset: 57.26%\n",
            "Accuracy on trainset: 81.90%\n",
            "Epoch [103/200], Loss: 0.7989\n",
            "Accuracy on testset: 56.28%\n",
            "Accuracy on trainset: 80.93%\n",
            "Epoch [104/200], Loss: 1.0351\n",
            "Accuracy on testset: 56.78%\n",
            "Accuracy on trainset: 82.96%\n",
            "Epoch [105/200], Loss: 0.9432\n",
            "Accuracy on testset: 56.87%\n",
            "Accuracy on trainset: 82.10%\n",
            "Epoch [106/200], Loss: 0.9709\n",
            "Accuracy on testset: 56.78%\n",
            "Accuracy on trainset: 82.21%\n",
            "Epoch [107/200], Loss: 1.0216\n",
            "Accuracy on testset: 57.27%\n",
            "Accuracy on trainset: 80.99%\n",
            "Epoch [108/200], Loss: 1.0440\n",
            "Accuracy on testset: 56.69%\n",
            "Accuracy on trainset: 82.45%\n",
            "Epoch [109/200], Loss: 1.0981\n",
            "Accuracy on testset: 56.60%\n",
            "Accuracy on trainset: 82.36%\n",
            "Epoch [110/200], Loss: 0.9635\n",
            "Accuracy on testset: 57.22%\n",
            "Accuracy on trainset: 82.74%\n",
            "Epoch [111/200], Loss: 0.8707\n",
            "Accuracy on testset: 57.36%\n",
            "Accuracy on trainset: 83.45%\n",
            "Epoch [112/200], Loss: 0.9848\n",
            "Accuracy on testset: 56.39%\n",
            "Accuracy on trainset: 81.68%\n",
            "Epoch [113/200], Loss: 0.8632\n",
            "Accuracy on testset: 57.31%\n",
            "Accuracy on trainset: 83.01%\n",
            "Epoch [114/200], Loss: 0.9732\n",
            "Accuracy on testset: 57.11%\n",
            "Accuracy on trainset: 83.38%\n",
            "Epoch [115/200], Loss: 0.8822\n",
            "Accuracy on testset: 57.40%\n",
            "Accuracy on trainset: 82.81%\n",
            "Epoch [116/200], Loss: 0.8102\n",
            "Accuracy on testset: 57.50%\n",
            "Accuracy on trainset: 83.87%\n",
            "Epoch [117/200], Loss: 0.9977\n",
            "Accuracy on testset: 56.83%\n",
            "Accuracy on trainset: 83.57%\n",
            "Epoch [118/200], Loss: 0.9012\n",
            "Accuracy on testset: 57.17%\n",
            "Accuracy on trainset: 83.07%\n",
            "Epoch [119/200], Loss: 1.0222\n",
            "Accuracy on testset: 57.04%\n",
            "Accuracy on trainset: 83.82%\n",
            "Epoch [120/200], Loss: 0.9412\n",
            "Accuracy on testset: 57.06%\n",
            "Accuracy on trainset: 83.60%\n",
            "Epoch [121/200], Loss: 0.9810\n",
            "Accuracy on testset: 57.05%\n",
            "Accuracy on trainset: 82.32%\n",
            "Epoch [122/200], Loss: 0.7614\n",
            "Accuracy on testset: 56.98%\n",
            "Accuracy on trainset: 83.49%\n",
            "Epoch [123/200], Loss: 1.3197\n",
            "Accuracy on testset: 57.51%\n",
            "Accuracy on trainset: 84.30%\n",
            "Epoch [124/200], Loss: 1.0228\n",
            "Accuracy on testset: 56.38%\n",
            "Accuracy on trainset: 83.26%\n",
            "Epoch [125/200], Loss: 0.8274\n",
            "Accuracy on testset: 56.67%\n",
            "Accuracy on trainset: 84.42%\n",
            "Epoch [126/200], Loss: 0.5759\n",
            "Accuracy on testset: 57.37%\n",
            "Accuracy on trainset: 84.55%\n",
            "Epoch [127/200], Loss: 0.5852\n",
            "Accuracy on testset: 57.80%\n",
            "Accuracy on trainset: 84.04%\n",
            "Epoch [128/200], Loss: 0.7898\n",
            "Accuracy on testset: 57.23%\n",
            "Accuracy on trainset: 84.79%\n",
            "Epoch [129/200], Loss: 0.8858\n",
            "Accuracy on testset: 56.70%\n",
            "Accuracy on trainset: 83.94%\n",
            "Epoch [130/200], Loss: 0.7906\n",
            "Accuracy on testset: 56.62%\n",
            "Accuracy on trainset: 84.64%\n",
            "Epoch [131/200], Loss: 0.9091\n",
            "Accuracy on testset: 56.63%\n",
            "Accuracy on trainset: 82.65%\n",
            "Epoch [132/200], Loss: 1.0650\n",
            "Accuracy on testset: 57.29%\n",
            "Accuracy on trainset: 84.60%\n",
            "Epoch [133/200], Loss: 0.9969\n",
            "Accuracy on testset: 56.97%\n",
            "Accuracy on trainset: 84.63%\n",
            "Epoch [134/200], Loss: 0.5196\n",
            "Accuracy on testset: 57.05%\n",
            "Accuracy on trainset: 85.07%\n",
            "Epoch [135/200], Loss: 0.8372\n",
            "Accuracy on testset: 57.24%\n",
            "Accuracy on trainset: 84.86%\n",
            "Epoch [136/200], Loss: 0.7130\n",
            "Accuracy on testset: 57.29%\n",
            "Accuracy on trainset: 84.24%\n",
            "Epoch [137/200], Loss: 0.9197\n",
            "Accuracy on testset: 57.36%\n",
            "Accuracy on trainset: 85.19%\n",
            "Epoch [138/200], Loss: 0.9159\n",
            "Accuracy on testset: 56.58%\n",
            "Accuracy on trainset: 85.00%\n",
            "Epoch [139/200], Loss: 0.8081\n",
            "Accuracy on testset: 56.39%\n",
            "Accuracy on trainset: 84.08%\n",
            "Epoch [140/200], Loss: 0.7011\n",
            "Accuracy on testset: 56.58%\n",
            "Accuracy on trainset: 84.78%\n",
            "Epoch [141/200], Loss: 1.0632\n",
            "Accuracy on testset: 56.90%\n",
            "Accuracy on trainset: 85.56%\n",
            "Epoch [142/200], Loss: 0.8018\n",
            "Accuracy on testset: 56.81%\n",
            "Accuracy on trainset: 85.37%\n",
            "Epoch [143/200], Loss: 0.9993\n",
            "Accuracy on testset: 57.19%\n",
            "Accuracy on trainset: 85.45%\n",
            "Epoch [144/200], Loss: 0.7817\n",
            "Accuracy on testset: 57.59%\n",
            "Accuracy on trainset: 85.73%\n",
            "Epoch [145/200], Loss: 0.7642\n",
            "Accuracy on testset: 57.20%\n",
            "Accuracy on trainset: 84.84%\n",
            "Epoch [146/200], Loss: 0.9598\n",
            "Accuracy on testset: 57.11%\n",
            "Accuracy on trainset: 85.35%\n",
            "Epoch [147/200], Loss: 0.8707\n",
            "Accuracy on testset: 56.73%\n",
            "Accuracy on trainset: 85.25%\n",
            "Epoch [148/200], Loss: 0.9606\n",
            "Accuracy on testset: 57.41%\n",
            "Accuracy on trainset: 86.05%\n",
            "Epoch [149/200], Loss: 0.8304\n",
            "Accuracy on testset: 57.79%\n",
            "Accuracy on trainset: 85.92%\n",
            "Epoch [150/200], Loss: 0.8372\n",
            "Accuracy on testset: 57.08%\n",
            "Accuracy on trainset: 83.51%\n",
            "Epoch [151/200], Loss: 0.7578\n",
            "Accuracy on testset: 57.48%\n",
            "Accuracy on trainset: 85.39%\n",
            "Epoch [152/200], Loss: 1.0249\n",
            "Accuracy on testset: 57.45%\n",
            "Accuracy on trainset: 86.05%\n",
            "Epoch [153/200], Loss: 0.7979\n",
            "Accuracy on testset: 57.45%\n",
            "Accuracy on trainset: 85.89%\n",
            "Epoch [154/200], Loss: 0.9456\n",
            "Accuracy on testset: 57.87%\n",
            "Accuracy on trainset: 85.75%\n",
            "Epoch [155/200], Loss: 1.0491\n",
            "Accuracy on testset: 57.79%\n",
            "Accuracy on trainset: 86.11%\n",
            "Epoch [156/200], Loss: 0.8770\n",
            "Accuracy on testset: 57.15%\n",
            "Accuracy on trainset: 87.21%\n",
            "Epoch [157/200], Loss: 1.0316\n",
            "Accuracy on testset: 57.43%\n",
            "Accuracy on trainset: 85.47%\n",
            "Epoch [158/200], Loss: 0.9558\n",
            "Accuracy on testset: 57.76%\n",
            "Accuracy on trainset: 87.19%\n",
            "Epoch [159/200], Loss: 0.8303\n",
            "Accuracy on testset: 57.36%\n",
            "Accuracy on trainset: 87.00%\n",
            "Epoch [160/200], Loss: 0.7434\n",
            "Accuracy on testset: 58.05%\n",
            "Accuracy on trainset: 86.99%\n",
            "Epoch [161/200], Loss: 0.6656\n",
            "Accuracy on testset: 57.55%\n",
            "Accuracy on trainset: 87.82%\n",
            "Epoch [162/200], Loss: 0.8083\n",
            "Accuracy on testset: 57.52%\n",
            "Accuracy on trainset: 86.28%\n",
            "Epoch [163/200], Loss: 0.7954\n",
            "Accuracy on testset: 57.91%\n",
            "Accuracy on trainset: 85.55%\n",
            "Epoch [164/200], Loss: 0.8111\n",
            "Accuracy on testset: 57.29%\n",
            "Accuracy on trainset: 87.30%\n",
            "Epoch [165/200], Loss: 0.9159\n",
            "Accuracy on testset: 57.72%\n",
            "Accuracy on trainset: 85.70%\n",
            "Epoch [166/200], Loss: 0.8417\n",
            "Accuracy on testset: 57.62%\n",
            "Accuracy on trainset: 87.52%\n",
            "Epoch [167/200], Loss: 1.0008\n",
            "Accuracy on testset: 57.36%\n",
            "Accuracy on trainset: 86.92%\n",
            "Epoch [168/200], Loss: 0.8759\n",
            "Accuracy on testset: 57.58%\n",
            "Accuracy on trainset: 87.51%\n",
            "Epoch [169/200], Loss: 0.8944\n",
            "Accuracy on testset: 57.97%\n",
            "Accuracy on trainset: 87.23%\n",
            "Epoch [170/200], Loss: 0.7521\n",
            "Accuracy on testset: 57.95%\n",
            "Accuracy on trainset: 87.12%\n",
            "Epoch [171/200], Loss: 0.9622\n",
            "Accuracy on testset: 56.98%\n",
            "Accuracy on trainset: 88.02%\n",
            "Epoch [172/200], Loss: 0.8936\n",
            "Accuracy on testset: 57.36%\n",
            "Accuracy on trainset: 87.14%\n",
            "Epoch [173/200], Loss: 0.8175\n",
            "Accuracy on testset: 57.54%\n",
            "Accuracy on trainset: 87.52%\n",
            "Epoch [174/200], Loss: 0.7274\n",
            "Accuracy on testset: 57.37%\n",
            "Accuracy on trainset: 87.59%\n",
            "Epoch [175/200], Loss: 0.9104\n",
            "Accuracy on testset: 57.48%\n",
            "Accuracy on trainset: 87.30%\n",
            "Epoch [176/200], Loss: 0.9249\n",
            "Accuracy on testset: 57.34%\n",
            "Accuracy on trainset: 87.49%\n",
            "Epoch [177/200], Loss: 0.6625\n",
            "Accuracy on testset: 57.58%\n",
            "Accuracy on trainset: 87.84%\n",
            "Epoch [178/200], Loss: 0.7552\n",
            "Accuracy on testset: 57.54%\n",
            "Accuracy on trainset: 88.59%\n",
            "Epoch [179/200], Loss: 0.8315\n",
            "Accuracy on testset: 56.87%\n",
            "Accuracy on trainset: 87.16%\n",
            "Epoch [180/200], Loss: 0.5254\n",
            "Accuracy on testset: 57.11%\n",
            "Accuracy on trainset: 88.19%\n",
            "Epoch [181/200], Loss: 0.6449\n",
            "Accuracy on testset: 56.97%\n",
            "Accuracy on trainset: 88.45%\n",
            "Epoch [182/200], Loss: 0.8936\n",
            "Accuracy on testset: 57.30%\n",
            "Accuracy on trainset: 87.91%\n",
            "Epoch [183/200], Loss: 0.5169\n",
            "Accuracy on testset: 57.29%\n",
            "Accuracy on trainset: 86.66%\n",
            "Epoch [184/200], Loss: 0.8939\n",
            "Accuracy on testset: 58.14%\n",
            "Accuracy on trainset: 88.32%\n",
            "Epoch [185/200], Loss: 1.0378\n",
            "Accuracy on testset: 57.37%\n",
            "Accuracy on trainset: 87.94%\n",
            "Epoch [186/200], Loss: 1.1787\n",
            "Accuracy on testset: 57.80%\n",
            "Accuracy on trainset: 88.58%\n",
            "Epoch [187/200], Loss: 0.7726\n",
            "Accuracy on testset: 57.23%\n",
            "Accuracy on trainset: 88.11%\n",
            "Epoch [188/200], Loss: 0.7984\n",
            "Accuracy on testset: 57.51%\n",
            "Accuracy on trainset: 88.51%\n",
            "Epoch [189/200], Loss: 0.8721\n",
            "Accuracy on testset: 57.97%\n",
            "Accuracy on trainset: 88.16%\n",
            "Epoch [190/200], Loss: 0.9037\n",
            "Accuracy on testset: 58.19%\n",
            "Accuracy on trainset: 88.46%\n",
            "Epoch [191/200], Loss: 0.8745\n",
            "Accuracy on testset: 57.62%\n",
            "Accuracy on trainset: 88.21%\n",
            "Epoch [192/200], Loss: 0.7479\n",
            "Accuracy on testset: 57.98%\n",
            "Accuracy on trainset: 87.70%\n",
            "Epoch [193/200], Loss: 0.6933\n",
            "Accuracy on testset: 57.86%\n",
            "Accuracy on trainset: 88.10%\n",
            "Epoch [194/200], Loss: 0.7727\n",
            "Accuracy on testset: 57.90%\n",
            "Accuracy on trainset: 89.39%\n",
            "Epoch [195/200], Loss: 0.8121\n",
            "Accuracy on testset: 57.79%\n",
            "Accuracy on trainset: 89.29%\n",
            "Epoch [196/200], Loss: 1.2404\n",
            "Accuracy on testset: 57.47%\n",
            "Accuracy on trainset: 88.45%\n",
            "Epoch [197/200], Loss: 0.7904\n",
            "Accuracy on testset: 58.16%\n",
            "Accuracy on trainset: 88.84%\n",
            "Epoch [198/200], Loss: 0.7872\n",
            "Accuracy on testset: 57.79%\n",
            "Accuracy on trainset: 88.80%\n",
            "Epoch [199/200], Loss: 0.9295\n",
            "Accuracy on testset: 57.69%\n",
            "Accuracy on trainset: 89.16%\n",
            "Epoch [200/200], Loss: 0.6742\n",
            "Accuracy on testset: 57.69%\n",
            "Accuracy on trainset: 89.16%\n",
            "Training finished!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 6), dpi=100)\n",
        "def generateModel(model_path, load_history = False):\n",
        "    model = LeNet()\n",
        "    if load_history == True:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    return model\n",
        "def train(model, train_features, train_labels, num_epochs, learning_rate, batch_size):\n",
        "    global test_features\n",
        "    print(device)\n",
        "    model.to(device) # 移动模型到cuda\n",
        "#     train_features = train_features.to(device) # 移动数据到cuda\n",
        "#     train_labels = train_labels.to(device)\n",
        "    dataset = TensorDataset(train_features, train_labels)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_accuracy = 0\n",
        "    # 定义损失函数和优化器\n",
        "    criterion = nn.CrossEntropyLoss()  # 交叉熵损失\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
        "    # 训练模型\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_features, batch_labels in data_loader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels.long())  # 计算损失\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if epoch % 1 == 0 or epoch == num_epochs - 1:\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                predictions = model(test_features)  # 添加通道维度并进行预测\n",
        "                predicted_labels = torch.argmax(predictions, dim=1)  # 获取预测标签\n",
        "            test_accuracy = (predicted_labels == test_labels).sum().item() / len(test_labels)\n",
        "            print(f'Accuracy on testset: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                predictions = model(train_features)  # 添加通道维度并进行预测\n",
        "                predicted_labels = torch.argmax(predictions, dim=1)  # 获取预测标签\n",
        "            accuracy = (predicted_labels == train_labels).sum().item() / len(train_labels)\n",
        "            print(f'Accuracy on trainset: {accuracy * 100:.2f}%')\n",
        "            model.train()\n",
        "\n",
        "            # train_accuracy_list.append(accuracy)\n",
        "            # test_accuracy_list.append(test_accuracy)\n",
        "            # epochs.append(epoch + 1)\n",
        "\n",
        "\n",
        "\n",
        "            #\n",
        "            # plt.subplot(1, 1, 1)\n",
        "            # try:\n",
        "            #     train_acc_lines.remove(train_acc_lines[0])  # 移除上一步曲线\n",
        "            #     val_acc_lines.remove(val_acc_lines[0])\n",
        "            # except Exception:\n",
        "            #     pass\n",
        "            # train_acc_lines = plt.plot(epochs, train_accuracy_list, 'r', lw=1)  # lw为曲线宽度\n",
        "            # val_acc_lines = plt.plot(epochs, test_accuracy_list, 'b', lw=1)\n",
        "            # plt.title(\"acc\")\n",
        "            # plt.xlabel(\"epoch\")\n",
        "            # plt.ylabel(\"acc\")\n",
        "            # plt.legend([\"train_acc\",\"test_acc\"])\n",
        "            # plt.show()\n",
        "            # plt.pause(0.1)  # 图片停留0.1s\n",
        "\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_features)  # 添加通道维度并进行预测\n",
        "        predicted_labels = torch.argmax(predictions, dim=1)  # 获取预测标签\n",
        "    test_accuracy = (predicted_labels == test_labels).sum().item() / len(test_labels)\n",
        "    print(f'Accuracy on testset: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(train_features)  # 添加通道维度并进行预测\n",
        "        predicted_labels = torch.argmax(predictions, dim=1)  # 获取预测标签\n",
        "    accuracy = (predicted_labels == train_labels).sum().item() / len(train_labels)\n",
        "    print(f'Accuracy on trainset: {accuracy * 100:.2f}%')\n",
        "\n",
        "    print('Training finished!')\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    model_name = type(model).__name__\n",
        "    model_file_name = f\"{model_name}_{current_time}_{test_accuracy * 100:.2f}\"\n",
        "    # 保存模型\n",
        "    torch.save(model.state_dict(), model_file_name)\n",
        "    last_name = model_file_name\n",
        "    plt.ioff()\n",
        "test_features = test_features.to(device)\n",
        "test_labels = test_labels.to(device)\n",
        "train_features = train_features.to(device)\n",
        "train_labels = train_labels.to(device)\n",
        "\n",
        "\n",
        "\n",
        "train(generateModel(\"LeNet_2023-09-22_06-44-19_54.28\",False), train_features, train_labels, 200, 0.001, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c2ecf6",
      "metadata": {
        "id": "08c2ecf6"
      },
      "outputs": [],
      "source": [
        "last_name = \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cxqkDIlpvfnY"
      },
      "id": "cxqkDIlpvfnY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}